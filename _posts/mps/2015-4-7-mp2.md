---
layout: default     
title: MP2&mdash; Text Categorization   
category: mps    
permalink: mps/mp2.html    
published: true    
---

This assignment is designed to help you practice with general steps in building a text categorization system. It  consists of four parts, including feature selection, building Naive Bayes classifier, kNN classifier, and classification performance evaluation, totaling 100 points.

<!--snippet-->

* **Part 1: [Feature Selection](#fv)** *(30 points)*: implement two basic feature selection methods, i.e., Information Gain and Chi Square test, for selecting high-quality text features;  
* **Part 2: [Naive Bayes](#nb)** *(20 points)*: build a Naive Bayes classifier for text document classification;
* **Part 3: [k Nearest Neighbor](#knn)** *(20 points)*: build a k Nearest Neighbor classifier for text document classification;
* **Part 4: [Classification performance evaluation](#eval)** *(30 points)*: evaluate the constructed classifiers with standard classification performance metrics, including precision, recall and F-measure.  

The deadline and submission guidance are explicitly described [here](#time).

# Data Set

We will use the same small Yelp review data set on the CS lab servers (e.g., *labunix01.cs.virginia.edu*, *labunix02.cs.virginia.edu*, *labunix03.cs.virginia.edu*):

	"/home/hw5x/TextMining/MP1/data/Yelp_Small" 

More details of this review data set can be found in our previous MP1 description.

Please reuse your implementation for MP1 as much as possible; and based that, you should be able to finish this assignment without too much effort.

# <a name="preparation"></a>Preparation: Creating the training and testing corpus

## Terminologies

We will keep using the same terminologies as we used in MP1. 

- **Document**: it refers to each individual Yelp review document contained in the json file. Therefore, one json file might contain multiple review documents.                
- **Restaurant**: it refers to a specific restaurant entity contained in the given review data set. It is uniquely identified by the corresponding Yelp ID; and one json for one restaurant.            
- **DF/IDF/TF/TTF/Cosine similarity**: the definitions for those metrics strictly follow those on our lecture slides, unless specifically defined below.

Next, let's go over several important concepts and techniques for basic text analysis.

## Basic text processing techniques

We will use the same text processing techniques as we have explored in MP1, including tokenization, stemming, normalization and stopword removal. Please find more details in our previous description of MP1.   

## Defining the classes

For each review document in this data set, it is associated with a numeric overall rating in the range of 1 to 5. We will treat the review documents with overall ratings **below 4 as negative** and **the rest as positive**. As a result, we have created a binary text classification problem. 

# <a name="fv"></a>Part1: Feature Selection (30pts)      

Based on the basic text processing steps we have taken in MP1, we have created a vector representation for every document in our collection. Next we will use two text feature selection methods, i.e., Information Gain and Chi Square, to further refine this representation.

The definition of Information Gain and Chi Square feature selector is as follows,

$$IG(t) = -\sum_c p(c)\log p(c) + p(t)\sum_c p(c|t)\log p(c|t) + p(\head t)\sum_c p(c|\head t)\log p(c|\head t) $$  

### 1.1 Understand Zipf's Law (20pts)

First, let's validate the Zipf's law with the provided Yelp review data set. This can be achieved by the following steps:

1. Process the text document according to the discussed steps above.
2. For each token, go over all the review documents containing it  (in both **train** and **test** folder), and accumulate its frequency in those documents, i.e., total term frequency (TTF).
3. Order the tokens by their TTF in a descending order.
4. Create a dot plot by treating each word's rank as x-axis and its TTF as y-axis. Please use log-log scale for this plot.

*Hint: basically, you can maintain a look-up table in memory while you are scanning through the documents, so that you only need to go through the corpus once to get the counts for all the tokens.*

From the resulting plot, can we find a strong linear relationship between the x and y axes in the log-log scale? If so, what is the slope of this linear relationship? To answer these questions, you can dump the data into excel and use the plot function there. (You can also use some scientific computing packages for curve fitting for this purpose.)          

Then change the counting statistics in the previous experiment from *total term frequency (TTF)* to *document frequency (DF)*, and perform the experiment again. According to new curve and corresponding slope and intercept of the linear interpretation, can you conclude which counting statistics, i.e., *TTF* v.s., *DF*, fits Zipf's law better on this data set? Can you give any explanation about why it fits the law better?

**What to submit**: 

1. Paste your implementation of text normalization module.
2. Two curves in log-log space generated above, with the corresponding slopes and intercepts of the linear interpretation results;
3. Your answers and thoughts to the above questions.

### 1.2 Construct a Controlled Vocabulary (10pts)

According to the procedure illustrated in our lecture slide, generate all the bigrams based on the resulting tokens from the pre-processing step (**only in the train folder**), and mix those bigrams with the unigrams as our initial controlled vocabulary.

Collect the DF statistics for all the N-grams (i.e., unigram and bigram) in this initial controlled vocabulary (**only in the train folder**), and find out the top 100 N-grams by DF. Compare this list with our initial stopword list and can you find some restaurant review specific stopwords? Merge those top 100 N-grams into the initial stopword list as our final stopword list.

In the meanwhile, filter out the N-grams with DF smaller than 50. All the rest N-grams will be used as our controlled vocabulary.

**What to submit**:

1. List of new stopwords you found specific to restaurant reviews.
2. The size of the resulting controlled vocabulary (i.e., total N-grams in it).
3. Top 50 and bottom 50 N-grams according to DF in the resulting controlled vocabulary, and their corresponding IDFs (**should be estimated only based on the train folder**).


### 1.3 Compute similarity between documents (20pts)

With the above automatically constructed controlled vocabulary, each review document can be represented as a N-gram vector. Specifically, each dimension in this vector space is defined by the mix of unigrams and bigrams defined in the controlled vocabulary; while the weight for each unigram/bigram in a review document is defined by TF-IDF. Specifically, we will use "**Sub-linear TF scaling**" to compute the normalized TF of each unigram/bigram in a review document. Note the IDF statistics should be **only computed based on the train folder**.

Using this document representation to encode all the review documents in the test folder. 

Under the folder of "/home/hw5x/TextMining/MP1/data/Yelp", there is a special json file named "query.json", which is manually crafted by the instructor. It contains five carefully selected restaurant reviews from and outside the provided corpus. Construct the vector space representations for these five reviews (as what you have done for those review documents in the test folder) and find out the most similar reviews to them from the test folder, where the similarity metric is defined as cosine similarity. 

**What to submit**:

1. Paste your implementation of cosine similarity computation.
2. For each document in the "query.json" file, list the top 3 most similar review documents (including Author, Content and Date) from the test folder and the corresponding cosine similarity.
3. Without reading the actual content in the "query.json" file, by simply judging from the content of the retrieved most similar review documents, can you tell which type of restaurants are specified in the query.json file (e.g., Indian food or Korean food)?
 
 
# <a name="slm"></a>Part2: Statistical Language Models (50pts)

### 2.1 Maximum likelihood estimation for statistical language models with proper smoothing (20pts)

Use all the review documents in the **train folder** to estimate a unigram language model $p\_u(w)$ and two bigram language models (with different smoothing methods specified below). When estimating the bigram language models, using linear interpolation smoothing and absolute discount smoothing based on the unigram language model $p\_u(w)$ to get two different bigram language models accordingly, i.e., $p^L\_b(w\_i|w\_{i-1})$ and $p^A\_b(w\_i|w\_{i-1})$. In linear interpolation smoothing, set the parameter $\lambda=0.9$; and in absolute discount smoothing, set the parameter $\delta=0.1$.

From the resulting two bigram language models, find the top 10 words that are most likely to follow the word "good", i.e., rank the words in a descending order by $p^L\_b(w|\unicode{x201C}good")$ and $p^A\_b(w|\unicode{x201C}good")$ and output the top 10 words. Are those top 10 words the same from these two bigram language models? Explain your observation.

*HINT: to reduce space complexity, you do not need to actually maintain a $V\times V$ array to store the counts and probabilities for the bigram language models. You can use a sparse data structure, e.g., hash map, to store the seen words/bigrams, and perform the smoothing on the fly, i.e., evoke some function calls to return the value of $p^L\_b(w|\unicode{x201C}good")$ and $p^A\_b(w|\unicode{x201C}good")$.* 

**What to submit**:

1. Paste your implementation of the linear interpolation smoothing and absolute discount smoothing.
2. The top 10 words selected from the corresponding two bigram language models.
3. Your explanation of the observations about the top words under those two bigram language models.

### 2.2 Generate text documents from a language model (15pts)

Fixing the sentence length to be 15, generate 10 sentences by sampling words from $p\_u(w)$, $p^L\_b(w\_i|w\_{i-1})$ and $p^A\_b(w\_i|w\_{i-1})$ respectively.

*HINT: you can use $p\_u(w)$ to generate the first word of a sentence and then sampling from the corresponding bigram language model when generating from $p^L\_b(w\_i|w\_{i-1})$ and $p^A\_b(w\_i|w\_{i-1})$.* 

**What to submit**:

1. Paste your implementation of the sampling procedure from a language model.
2. The 10 sentences generated from $p\_u(w)$, $p^L\_b(w\_i|w\_{i-1})$ and $p^A\_b(w\_i|w\_{i-1})$ accordingly, and the corresponding likelihood given by the used language model.

### 2.3 Language model evaluation (15pts)

Compute perplexity of the previously estimated language models, i.e., $p\_u(w)$, $p^L\_b(w\_i|w\_{i-1})$ and $p^A\_b(w\_i|w\_{i-1})$, on all the review documents in the **test folder**.

The perplexity metric defined in our lecture slide is for one document (copied below). Follow that definition to compute perplexity for every review document in the test folder and compute the mean and standard deviation of the  resulting perplexities.

$PP(w\_1,w\_2,\dots,w\_n) = \sqrt[n]{\frac{1}{\prod\_{i=1}^n p(w\_i|w\_{i-1},\dots,w\_{i-N+1})}}$

where $d=w\_1,w\_2,\dots,w\_n$, i.e., a text sequence from testing document $d$ of length $n$; and the likelihood is computed under an N-gram language model.

*NOTE: to smooth the unseen words in the test folder, use additive smoothing to smooth the unigram language model $p\_u(w)$ by setting the parameter $\delta=0.1$. Then use the smoothed $\hat p\_u(w)$ to smooth $p^L\_b(w\_i|w\_{i-1})$ and $p^A\_b(w\_i|w\_{i-1})$.*

**What to submit**:

1. Paste your implementation of the perplexity calculation of a language model.
2. Report the mean and standard deviation of the perplexities for $\hat p\_u(w)$, $\hat p^L\_b(w\_i|w\_{i-1})$ and $\hat p^A\_b(w\_i|w\_{i-1})$ on the test folder.     
3. Can you conclude which language model predicts the data in test folder better? And why?

# <a name="time"></a>Deadlines & How to submit

We have two deadlines: Part1 is due on **Feb. 12th 11:55pm**; and Part2 is due on **Feb. 19th 11:55pm**. Therefore, you have in total 14 days to finish this MP. The late policy for all our homework has been carefully discussed in the course syllabus.

Two separated collab assignment pages have been created for this MP. Please submit your written report strictly following the requirement specified above. The report **must be in PDF** format.   


# <a name="solution"></a>Sample solution

The following sample solution is provided by Mohammad Al Boni.

* Part 1: Vector Space Model, [Report]({{site.baseurl}}/docs/codes/MP1-Part1-Mohammad.pdf)
* Part 2: Statistical Language Models, [Report]({{site.baseurl}}/docs/codes/MP1-Part2-Mohammad.pdf)
* [Java Implementation]({{site.baseurl}}/docs/codes/MP1-solution-Mohammad.zip), [Presentation slides]({{site.baseurl}}/docs/codes/MP1-Mohammad.pptx)

The following derivation of $S$ in absolute discount is provided by Lin Gong.

* Proof of $S$ equals to the word types (i.e., unique words) following $w\_{i-1}$, [Proof]({{site.baseurl}}/docs/codes/MP1-Part2-Lin-AbsoluteDiscount.pdf)
  