---
layout: default     
title: MP3&mdash; Hidden Markov Model for Port-of-speech Tagging     
category: mps    
permalink: mps/mp3.html    
published: true    
---

This assignment is designed to help you get familiar with supervised Hidden Markov Model for Port-of-speech Tagging. It consists of two parts, totaling 50 points. This is a bonus homework assignment and you will have a longer time period to finish it.

<!--snippet-->

* **Part 1: [Parameter Estimation](#est)** *(20 points)*: using maximum likelihood estimator to estimate the model parameters in an Hidden Markov Model (HMM);
* **Part 2: [Tag Sequence Inference](#inf)** *(30 points)*: implementing Viterbi algorithm to perform the maximum a posterior inference and retrieve the best tag sequence for a given input text sentence.

The deadline and submission guidance are explicitly described [here](#time).

# Data Set

The instructor has prepared a small collection of Penn Treebank tagged documents for Part of Speech (in total 199 tagged documents). The data set can be downloaded [here]({{site.baseurl}}/docs/codes/postag.zip). 

Originally, each of the texts was run through PARTS (Ken Church's stochastic part-of-speech tagger) and then corrected by a human annotator. The square brackets surrounding phrases in the texts are the output of a stochastic NP parser that is part of PARTS and are best ignored. Words are separated from their part-of-speech tag by a forward slash. In cases of uncertainty concerning the proper part-of-speech tag, words are given alternate tags, which are separated from one another by a vertical bar.

The definition of tags can be found [here](http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html). 

*Note: as in Penn Treebank the definition of tags might change over time, you might found some tags that are not defined in the latest Penn Treebank list. And since this is only a small subset of Penn Treebank annotated corpus, many tags might not occur in this data set. Therefore, you only need to consider the tags occurred in this data set.*

# Sample implementation

Because this is a bonus homework assignment, we do not provide you sample implementation. But you can follow our sample implementation in MP1 for loading files from a directory, parsing text input, and using hashmap for language models.  

# <a name="preparation"></a>Preparation: Hidden Markov Model

![]({{site.baseurl}}/docs/codes/HMM.PNG)                                 
Figure 1. Graphical representation of Hidden Markov Model

A Hidden Markov model captures the joint probability between a word sequence and the corresponding tag sequence ($p(w\_1,\dots,w\_n,t\_1,\dots,t\_n)$) via making two independence assumptions:               
1. Current tag $t\_i$ only depends on previous $k$ tags:               
	$$p(t\_1,\dots,t\_n)=\prod^n\_{i=1} p(t\_i|t\_{i-1},\dots,t\_{i-k})$$           
2. Each word in the sequence depends only on its corresponding tag
	$$p(w\_1,\dots,w\_n|t\_1,\dots,t\_n)=\prod^n\_{i=1} p(w\_i|t\_{i})$$

In particular, $p(t\_i|t\_{i-1},\dots,t\_{i-k})$ specifies the *transition probability* among consecutive tags, and $ p(w\_i|t\_{i})$ specifies the *emission probability* from a tag to a word. In this assignment, we will only use first-order HMM, i.e., current tag only depends on previous one tag ($k=1$). And to make the implementation unified, we will add a dummy tag *START* at the beginning of each sentence: it does not generate any word, it can only transit to other tags, and no transition will go to this tag.      

# <a name="est"></a>Part1: Parameter Estimation (20pts)      

We will use Maximum Likelihood estimator for parameter estimation in HMM. Specifically, the transition and emission probabilities can be estimated as follows,

$$p(t_i|t_j)\propto c(t_j,t_i)+\delta$$
$$p(w_i|t_j)\propto c(w_i,t_j)+\sigma$$

### 1.1 Understand Zipf's Law (20pts)

First, let's validate the Zipf's law with the provided Yelp review data set. This can be achieved by the following steps:

1. Process the text document according to the discussed steps above.
2. For each token, go over all the review documents containing it  (in both **train** and **test** folder), and accumulate its frequency in those documents, i.e., total term frequency (TTF).
3. Order the tokens by their TTF in a descending order.
4. Create a dot plot by treating each word's rank as x-axis and its TTF as y-axis. Please use log-log scale for this plot.

*Hint: basically, you can maintain a look-up table in memory while you are scanning through the documents, so that you only need to go through the corpus once to get the counts for all the tokens.*

From the resulting plot, can we find a strong linear relationship between the x and y axes in the log-log scale? If so, what is the slope of this linear relationship? To answer these questions, you can dump the data into excel and use the plot function there. (You can also use some scientific computing packages for curve fitting for this purpose.)          

**What to submit**: 

1. Paste your implementation of text normalization module.
2. The generated curve for Zipf's law in log-log space, with the corresponding slope and intercept of the linear interpolation results.
 
 
# <a name="inf"></a>Part2: Viterbi Algorithm for Posterior Inference (30pts)

### 2.1 Maximum likelihood estimation for statistical language models with proper smoothing (20pts)

Use all the review documents in the **train folder** to estimate a unigram language model $p\_u(w)$ and two bigram language models (with different smoothing methods specified below). When estimating the bigram language models, using linear interpolation smoothing and absolute discount smoothing based on the unigram language model $p\_u(w)$ to get two different bigram language models accordingly, i.e., $p^L\_b(w\_i|w\_{i-1})$ and $p^A\_b(w\_i|w\_{i-1})$. In linear interpolation smoothing, set the parameter $\lambda=0.9$; and in absolute discount smoothing, set the parameter $\delta=0.1$.

From the resulting two bigram language models, find the top 10 words that are most likely to follow the word "good", i.e., rank the words in a descending order by $p^L\_b(w|\unicode{x201C}good")$ and $p^A\_b(w|\unicode{x201C}good")$ and output the top 10 words. Are those top 10 words the same from these two bigram language models? Explain your observation.

*HINT: to reduce space complexity, you do not need to actually maintain a $V\times V$ array to store the counts and probabilities for the bigram language models. You can use a sparse data structure, e.g., hash map, to store the seen words/bigrams, and perform the smoothing on the fly, i.e., evoke some function calls to return the value of $p^L\_b(w|\unicode{x201C}good")$ and $p^A\_b(w|\unicode{x201C}good")$.* 

**What to submit**:

1. Paste your implementation of the linear interpolation smoothing and absolute discount smoothing.
2. The top 10 words selected from the corresponding two bigram language models.
3. Your explanation of the observations about the top words under those two bigram language models.


# <a name="time"></a>Deadlines & How to submit

You will have four weeks on this assignment, i.e., the due date is May 2nd, 11:55pm. A submission site will be created on collab. Please submit your written report strictly following the requirement specified above. The report **must be in PDF** format.   

