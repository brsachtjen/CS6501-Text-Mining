---
layout: default     
title: MP3&mdash; Hidden Markov Model for Port-of-speech Tagging     
category: mps    
permalink: mps/mp3.html    
published: true    
---

This assignment is designed to help you get familiar with supervised Hidden Markov Model for Port-of-speech Tagging. It consists of two parts, totaling 50 points. This is a bonus homework assignment and you will have a longer time period to finish it.

<!--snippet-->

* **Part 1: [Parameter Estimation](#est)** *(15 points)*: using maximum likelihood estimator to estimate the model parameters in an Hidden Markov Model (HMM);
* **Part 2: [Tag Sequence Inference](#inf)** *(35 points)*: implementing Viterbi algorithm to perform the maximum a posterior inference and retrieve the best tag sequence for a given input text sentence.

The deadline and submission guidance are explicitly described [here](#time).

# Data Set

The instructor has prepared a small collection of Penn Treebank tagged documents for Part of Speech (in total 199 tagged documents). The data set can be downloaded [here]({{site.baseurl}}/docs/codes/postag.zip). 

Originally, each of the texts was run through PARTS (Ken Church's stochastic part-of-speech tagger) and then corrected by a human annotator. The square brackets surrounding phrases in the texts are the output of a stochastic NP parser that is part of PARTS and are best ignored. Words are separated from their part-of-speech tag by a forward slash. In cases of uncertainty concerning the proper part-of-speech tag, words are given alternate tags, which are separated from one another by a vertical bar.

The definition of tags can be found [here](http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html). 

*Note: as in Penn Treebank the definition of tags is changing over time, you might find some tags that are not defined in the latest Penn Treebank list. And since this is only a small subset of Penn Treebank annotated corpus, many tags might not occur in this data set. Therefore, you only need to consider the tags occurred in this provided data set.*

# Sample implementation

Because this is a bonus homework assignment, we do not provide you any sample implementation. But you can follow our sample implementation in MP1 for loading files from a directory, parsing text input, and using hashmap for language models to finish this assignment.  

# <a name="preparation"></a>Preparation: Hidden Markov Model

![]({{site.baseurl}}/docs/codes/HMM.PNG)                                 
Figure 1. Graphical representation of Hidden Markov Model

A Hidden Markov model captures the joint probability between a word sequence and the corresponding tag sequence, i.e., $p(w\_1,\dots,w\_n,t\_1,\dots,t\_n)$, via making the following two independence assumptions:               
1. Current tag $t\_i$ only depends on previous $k$ tags:               
	$$p(t\_1,\dots,t\_n)=\prod^n\_{i=1} p(t\_i|t\_{i-1},\dots,t\_{i-k})$$           
2. Each word in the sequence depends only on its corresponding tag
	$$p(w\_1,\dots,w\_n|t\_1,\dots,t\_n)=\prod^n\_{i=1} p(w\_i|t\_{i})$$

In particular, $p(t\_i|t\_{i-1},\dots,t\_{i-k})$ specifies the *transition probability* among consecutive tags, and $ p(w\_i|t\_{i})$ specifies the *emission probability* from a tag to a word. In this assignment, we will only use the first-order HMM, i.e., current tag only depends on previous one tag ($k=1$). And to make the implementation unified, we will add a dummy tag *START* at the beginning of each sentence: it does not generate any word, it can only transit to other tags, and no tag can transit to this tag.      

# <a name="est"></a>Part1: Parameter Estimation (15pts)      

We will use Maximum Likelihood estimator for parameter estimation in HMM. Specifically, the transition and emission probabilities can be estimated as follows,

$$p(t\_i|t\_j)\propto c(t\_j,t\_i)+\delta$$                     
$$p(w\_i|t\_j)\propto c(w\_i,t\_j)+\sigma$$

where $c(t\_j,t\_i)$ denote the count that tag $t\_i$ follows $t\_j$ in training corpus, $c(w\_i,t\_j)$ denotes the count that word $w\_i$ is assigned to tag $t\_j$, $\delta$ and $\sigma$ are the smoothing parameters for transition and emission probabilities estimation. In this assignment, we set $\delta=0.5$ and $\sigma=0.1$. 

**What to submit**: 

1. Paste your implementation of maximum likelihood estimation for HMM. (10pts)
2. List the top 10 words under the tag "NN" and top 10 most probable tags after the tag "VB". (5pts) 
 
# <a name="inf"></a>Part2: Viterbi Algorithm for Posterior Inference (35pts)

The posterior inference in HMM, 
$$argmax\_{t\_1,\dots,t\_n} p(w\_1,\dots,w\_n|t\_1,\dots,t\_n)p(t\_1,\dots,t\_n)$$
can be efficiently performed via the Viterbi algorithm, which essentially is dynamic programming. To implement this algorithm, you need to create a special data structure called Trellis, which stores the best tag sequence for $w\_1,w\_2\dots,w\_i$ that ends with tag $t\_j$ in its cell $T[j][i]$. Detailed description of this algorithm can be found in our lecture slides. 

The output of Viterbi algorithm is the best tag sequence $t\_1,\dots,t\_n$ for the input word sequence $w\_1,\dots,w\_n$.

**What to submit**:

1. Paste your implementation of the Viterbi algorithm for HMM. (15pts)
2. Perform 5-fold cross-validation on the provided Penn Treebank annotation set, use tag prediction accuracy/precision/recall on test set as your evaluation metric. Report overall accuracy/precision/recall, and precision/recall for the tags of "NN", "VB", "ADJ", and "NNP". (10pts)
3. Tune the smoothing parameter of $\delta$ and $\sigma$ and repeat your cross-validation, figure out the best configuration for $\delta$ and $\sigma$ on this data set. (10pts)

# <a name="time"></a>Deadlines & How to submit

You will have four weeks on this assignment, i.e., the due date is May 2nd, 11:55pm. A submission site will be created on collab. Please submit your written report strictly following the requirement specified above. The report **must be in PDF** format.   

