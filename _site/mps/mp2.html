<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      MP2&mdash; Text Categorization &middot; CS 6501: Text Mining
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/~hw5x/Course/Text-Mining-2015-Spring/_site/css/poole.css">
  <link rel="stylesheet" href="/~hw5x/Course/Text-Mining-2015-Spring/_site/css/syntax.css">
  <link rel="stylesheet" href="/~hw5x/Course/Text-Mining-2015-Spring/_site/css/lanyon.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x146" href="/~hw5x/Course/Text-Mining-2015-Spring/_site/UVA_Rotunda_Logo.png">
  <link rel="shortcut icon" sizes="32x32" href="/~hw5x/Course/Text-Mining-2015-Spring/_site/uva_rotunda.jpg">  
</head>


  <body>

    <script type="text/javascript"
      src="https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script> 
    
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']],
                         displayMath: [['\\[','\\]'], ['$$','$$']]}});
    </script>

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <p>Formula for this course: <br><b>Text Mining = Data Mining + Text Data</b></p>
  </div>

  <nav class="sidebar-nav">
    <a class="sidebar-nav-item" href="/~hw5x/Course/Text-Mining-2015-Spring/_site">Home</a>

    

    
    
      
    
      
        
          <a class="sidebar-nav-item" href="/~hw5x/Course/Text-Mining-2015-Spring/_site/assignments/">Homework</a>
        
      
    
      
        
      
    
      
        
          <a class="sidebar-nav-item" href="/~hw5x/Course/Text-Mining-2015-Spring/_site/lectures/">Lectures</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="/~hw5x/Course/Text-Mining-2015-Spring/_site/presentation/">Paper Presentation</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="/~hw5x/Course/Text-Mining-2015-Spring/_site/project/">Course Project</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="/~hw5x/Course/Text-Mining-2015-Spring/_site/quizzes/">Quizzes</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="/~hw5x/Course/Text-Mining-2015-Spring/_site/resources/">Resources</a>
        
      
    
  </nav>

  <div class="sidebar-item">
  	<p>
  		Currently v1.0.0
  	</p>
    <p>
      &copy; 2015. All rights reserved. <a href="http://www.cs.virginia.edu/">CS@UVa</a>
    </p>
  </div>
</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap">
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title">
            <a href="/~hw5x/Course/Text-Mining-2015-Spring/_site" title="Home">CS 6501: Text Mining</a>
            <small>Spring 2015 &middot; <a href="http://www.cs.virginia.edu/">CS@UVa</a></small>
          </h3>
        </div>
      </div>

      <div class="container content">
        <p>This assignment is designed to help you practice with general steps in building a text categorization system. It  consists of four parts, including feature selection, building Naive Bayes classifier, kNN classifier, and classification performance evaluation, totaling 100 points.</p>

<!--snippet-->

<ul>
<li><strong>Task 1: <a href="#fv">Feature selection</a></strong> <em>(30 points)</em>: implement two basic feature selection methods, i.e., Information Gain and Chi Square test, for selecting high-quality text features;<br></li>
<li><strong>Task 2: <a href="#nb">Naive Bayes</a></strong> <em>(20 points)</em>: build a Naive Bayes classifier for text document classification;</li>
<li><strong>Task 3: <a href="#knn">k Nearest Neighbor</a></strong> <em>(25 points)</em>: build a k Nearest Neighbor classifier for text document classification;</li>
<li><strong>Task 4: <a href="#eval">Classification performance evaluation</a></strong> <em>(25 points)</em>: evaluate the constructed classifiers with standard classification performance metrics, including precision, recall and F-measure.<br></li>
</ul>

<p>The deadline and submission guidance are explicitly described <a href="#time">here</a>.</p>

<h1>Data Set</h1>

<p>We will use the same small Yelp review data set as we used for MP1 on the CS lab servers (e.g., <em>labunix01.cs.virginia.edu</em>, <em>labunix02.cs.virginia.edu</em>, <em>labunix03.cs.virginia.edu</em>):</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">&quot;/home/hw5x/TextMining/MP1/data/Yelp_Small&quot; 
</code></pre></div>
<p>More details of this review data set can be found in our previous <a href="/%7Ehw5x/Course/Text-Mining-2015-Spring/_site/mps/mp1.html">MP1 description</a>.</p>

<p>Please reuse your implementation for MP1 as much as possible; and based that, you should be able to finish this assignment without too much effort. </p>

<p><em>HINT: since we have provided a sample solution provided by Mohammad Al Boni, you can refer to that sample implementation to solve the following MP2 tasks.</em> </p>

<h1><a name="preparation"></a>Preparation: Creating the training and testing corpus</h1>

<h2>Terminologies</h2>

<p>We will keep using the same terminologies as we used in MP1. </p>

<ul>
<li><strong>Document</strong>: it refers to each individual Yelp review document contained in the json file. Therefore, one json file might contain multiple review documents.<br></li>
<li><strong>Restaurant</strong>: it refers to a specific restaurant entity contained in the given review data set. It is uniquely identified by the corresponding Yelp ID; and one json for one restaurant.<br></li>
<li><strong>DF/IDF/TF/TTF/Cosine similarity</strong>: the definitions for those metrics strictly follow those on our lecture slides, unless specifically defined below.</li>
</ul>

<h2>Basic text processing techniques</h2>

<p>We will use the same text processing techniques as we have explored in MP1, including tokenization, stemming, normalization and stopword removal (please use the standard <a href="http://jmlr.org/papers/volume5/lewis04a/a11-smart-stop-list/english.stop">Smart system&#39;s stopword list</a>). More action details can be found in our previous <a href="/%7Ehw5x/Course/Text-Mining-2015-Spring/_site/mps/mp1.html">description of MP1</a>. Note, in all the following tasks, we will only use <strong>unigram as our features</strong> and <strong>json files in the train folder</strong>.   </p>

<h2>Defining the classes</h2>

<p>For each review document in this data set, it is associated with a numeric overall rating in the range of 1 to 5. We will treat the review documents with overall ratings <strong>below 4 as negative (labeled as 0)</strong> and <strong>the rest as positive (labeled as 1)</strong>. As a result, we have created a binary text classification problem. </p>

<h1><a name="fv"></a>Task 1: Feature Selection (30pts)</h1>

<p>Based on the basic text processing steps we have taken in MP1, we have created the vector representation for every review document in our collection. Next we will implement two most effective text feature selection methods, i.e., Information Gain and Chi Square, to further refine this representation.</p>

<p>The definitions of Information Gain and Chi Square feature selector are as follows (please also refer to our lecture slides),</p>

<p>$$ IG(t) = -\sum_{y} p(y)\log p(y) + p(t)\sum_{y} p(y|t)\log p(y|t) + p(\bar t)\sum_{y} p(y|\bar t)\log p(y|\bar t) $$,
where $p(y=1|t)$ is the probability that we get a positive document where we observe the term t, and $p(y=1|\bar t)$ is the probability that we get a positive document where we do not observe the term t.</p>

<p><strong>Note</strong>: here we need to define $0\log 0=0$ to avoid zero probabilities in entropy computation.  </p>

<p>$$\chi^2(t) = \frac{(A+B+C+D)(AD-BC)}{(A+C)(B+D)(A+B)(C+D)} $$, 
where A is the number of positive documents containing term t, B is the number of positive documents not containing term t, C is the number of negative documents containing term t, and D is the number of positive documents not containing term t.</p>

<p>Before we apply these two feature selection methods, we will perform prefiltering of features by DF. We will use the same threshold <strong>50</strong> as in MP1, i.e., removing the terms with DF smaller than 50. </p>

<p>As required in Chi Square, we will use the threshold <strong>3.841</strong> to filter out the insignificant terms, i.e., removing the terms whose $\chi^2(t)&lt;3.841$.</p>

<p>After this filtering steps, rank the left features by their $IG(t)$ value and $\chi^2(t)$ value in descending orders accordingly. We will choose the top 5000 words from each of these two ordered lists as our candidate features. If we don&#39;t have 5000 features left in Chi Square selection, return whatever you got after filtering.</p>

<p>Before moving onto the next task, merge the feature lists you have got from Information Gain and Chi Square, i.e., take the union of these two sets of top 5000 features, and use the resulting list as the finalized features (i.e., controlled vocabulary) for the following tasks. Use this controlled vocabulary to construct the vector representation for all the review documents in the train folder, and <strong>ignore</strong> the documents with less or equal to five non-zero features (i.e., length of the resulting sparse vector less or equal to 5). We will refer to the resulting document collection as <strong>corpus</strong> in our following descriptions.</p>

<p><strong>What to submit</strong>:</p>

<ol>
<li>List of the top 20 words selected by Information Gain and Chi Square accordingly. What is the size of your finalized feature set after merging? How many review documents are there in the resulting corpus after feature selection? (10 pts).</li>
<li>Paste your code for Information Gain and Chi Square computation (2$\times$10 pts).</li>
</ol>

<h1><a name="nb"></a>Task 2: Naive Bayes (20pts)</h1>

<h3>2.1 Training Naive Bayes with Maximum a Posterior estimator (10pts)</h3>

<p>As we discussed in the course lecture, Naive Bayes classifier for text categorization problem can be considered as estimating independent N-gram language models for each class. In this assignment, we will use unigram language model with additive smoothing (set $\delta=0.1$) to estimate parameters in a Naive Bayes text classifier.</p>

<p>You are suggested to reuse your implementation in MP1 for unigram language models in this task. </p>

<p><em>HINT: you only need to estimate two unigram language models for positive and negative classes accordingly, and $p(c)$ is simply the ratio of the two classes in the corpus.</em> </p>

<p><strong>What to submit</strong>:</p>

<ol>
<li>Use the whole corpus to estimate a Naive Bayes classifier. Compute the log ratio between $p(w|c=1)$ and $p(w|c=0)$, i.e., $\log\frac{p(w|c=1)}{p(w|c=0)}$, and rank the features in a descending order with respect to this  log ratio. List the top 20 and bottom 20 words from the ranked list. Do they make sense to you in terms of distinguishing the positive opinion from negative opinion? (10 pts).</li>
</ol>

<h3>2.2 Naive Bayes as a linear classifier (10pts)</h3>

<p>As discussed in our lecture slides, Naive Bayes for binary classification boils down to be a linear classification model:</p>

<p>$$f(X)=\log\frac{p(y=1|X)}{p(y=0|X)})=\log\frac{p(c=1)}{p(c=0)} + \sum_x(\log p(x|y=1) - \log p(x|y=0))$$, and $\bar y=sgn(f(X))$
where $sgn(x)=1$ if $x\ge1$, otherwise 0. </p>

<p>Implement this prediction function with your Naive Bayes classifier. It should take a document as input and output its predicted class label. </p>

<p><strong>What to submit</strong>:
1. Compute $f(X)$ over all the documents in the corpus and rank them in a descending order with respect to $f(X)$. Plot the Precision-Recall curve with respect ranked list. Please refer to the lecture slides for the instructions of plotting this curve.</p>

<h1><a name="knn"></a>Task 3: k Nearest Neighbor (25pts)</h1>

<p>In our lecture discussion, we have introduced a highly efficient approximation method for retrieving the nearest neighbors to the given input instance, i.e., <a href="http://en.wikipedia.org/wiki/Random_projection">random projection</a>. We will implement this method to speed up our kNN computation.</p>

<p>In our kNN classifier, we will use TF-IDF as feature value in vector representation and cosine similarity between vectors as distance metric. We will follow the same definition of TF-IDF as we used in MP1, i.e., sub-linear TF scaling. *Note in Naive Bayes classifier, we only need raw TF as our feature vector. * </p>

<p>Denote the size of our finalized controlled vocabulary as $m$, we will create a $l$-bits hash code for every review document in the corpus via random projection. The details of how to perform random projection has been discussed in our lecture slides. Briefly, we will create $l$ different $m$-dimensional random vectors, and each dimension of these random vectors falls into the range of [-1,1]. For an input review document, we will take the inner product of its vector representation with those $l$ random vectors accordingly; and use the sign function to encode each bit as 0 or 1, i.e., $H(x)=[sgn(r_1^Tx),sgn(r_2^Tx),\dots,sgn(r_l^Tx)]$.</p>

<p>Please keep in mind that those $l$ random vectors should be fixed and shared among all the review documents. In other words, you should first create those $l$ random vectors, store them, and keep using them for all the incoming documents when computing their hashing values. In addition, random projection only helps you narrow down the search. Unless there are less than k documents in the resulting hash bucket, you still need to compute cosine similarity between the input document and all the documents in the resulting hash bucket to retrieve the k nearest neighbors to the input document.   </p>

<p><strong>What to submit</strong>:<br>
1. Copy and paste your implementation of random projection. (15pts)<br>
2. Use your implemented random projection (set $l=10$) to retrieve the k nearest documents (set $k=5$) to those five sample review documents listed in the query json file in MP1. Report the running time for brute-force k nearest neighbor search and approximate k nearest neighbor search by random projection. Can we still find reasonable results with this approximation, i.e., the retrieved documents help you determine the cuisine mentioned in the query document? (10pts)</p>

<h1><a name="eval"></a>Task 4: Classification performance evaluation (25pts)</h1>

<p>In all our previous description, we used the same corpus for training and testing purpose. Now, we will perform text classification performance evaluation in a more rigorous way, i.e., using cross validation.</p>

<p>We will split the corpus into 10 folds by randomly assigning the documents an integer ID between 1 to 10. For each round, we will reserve one fold for testing and rest for training. Detailed procedures can be found in our lecture slides.</p>

<p>In each round, train and test Naive Bayes and kNN on the same corpus separation. In kNN, set $l=10$ and $k=5$. </p>

<p><strong>What to submit</strong>:<br>
1. Report average precision, recall and F1 measure of Naive Bayes and kNN over these 10 fold cross validation. (15pts)<br>
2. Perform paired two-sample t-test to verify if one classifier is consistently better than another (set the confidence level to be 95%)? If so, which one is better? (10pts)</p>

<h1>Bonus Task: Explore the best configuration of $l$ and $k$ (15pts)</h1>

<p>In our random projection based kNN implementation, we have two hyper-parameters, i.e., projection hash code length $l$ and number of nearest neighbors $k$. We set them arbitrarily. Use cross-validation as model selection method to choose the best configuration of $l$ and $k$.</p>

<p><strong>What to submit</strong>:<br>
1. Your design of experiment that helps you retrieve the best configuration of $l$ and $k$. And the optimal $l$ and $k$ you have found. (15pts) </p>

<h1><a name="time"></a>Deadlines &amp; How to submit</h1>

<p>MP2 is due on <strong>Apr. 16th 11:55pm</strong>*. You have in total 7 days to finish this MP. The late policy for all our homework assignments has been carefully discussed in the course syllabus.</p>

<p>Collab assignment page has been created for this MP. Please submit your written report strictly following the requirement specified above. The report <strong>must be in PDF</strong> format.    </p>

      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <script>
      (function(document) {
        var toggle = document.querySelector('.sidebar-toggle');
        var sidebar = document.querySelector('#sidebar');
        var checkbox = document.querySelector('#sidebar-checkbox');

        document.addEventListener('click', function(e) {
          var target = e.target;

          if(!checkbox.checked ||
             sidebar.contains(target) ||
             (target === checkbox || target === toggle)) return;

          checkbox.checked = false;
        }, false);
      })(document);
    </script>
  </body>
</html>
