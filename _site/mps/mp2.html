<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      MP2&mdash; Text Categorization &middot; CS 6501: Text Mining
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/~hw5x/Course/Text-Mining-2015-Spring/_site/css/poole.css">
  <link rel="stylesheet" href="/~hw5x/Course/Text-Mining-2015-Spring/_site/css/syntax.css">
  <link rel="stylesheet" href="/~hw5x/Course/Text-Mining-2015-Spring/_site/css/lanyon.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x146" href="/~hw5x/Course/Text-Mining-2015-Spring/_site/UVA_Rotunda_Logo.png">
  <link rel="shortcut icon" sizes="32x32" href="/~hw5x/Course/Text-Mining-2015-Spring/_site/uva_rotunda.jpg">  
</head>


  <body>

    <script type="text/javascript"
      src="https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script> 
    
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']],
                         displayMath: [['\\[','\\]'], ['$$','$$']]}});
    </script>

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <p>Formula for this course: <br><b>Text Mining = Data Mining + Text Data</b></p>
  </div>

  <nav class="sidebar-nav">
    <a class="sidebar-nav-item" href="/~hw5x/Course/Text-Mining-2015-Spring/_site">Home</a>

    

    
    
      
    
      
        
          <a class="sidebar-nav-item" href="/~hw5x/Course/Text-Mining-2015-Spring/_site/assignments/">Homework</a>
        
      
    
      
        
      
    
      
        
          <a class="sidebar-nav-item" href="/~hw5x/Course/Text-Mining-2015-Spring/_site/lectures/">Lectures</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="/~hw5x/Course/Text-Mining-2015-Spring/_site/presentation/">Paper Presentation</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="/~hw5x/Course/Text-Mining-2015-Spring/_site/project/">Course Project</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="/~hw5x/Course/Text-Mining-2015-Spring/_site/quizzes/">Quizzes</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="/~hw5x/Course/Text-Mining-2015-Spring/_site/resources/">Resources</a>
        
      
    
  </nav>

  <div class="sidebar-item">
  	<p>
  		Currently v1.0.0
  	</p>
    <p>
      &copy; 2015. All rights reserved. <a href="http://www.cs.virginia.edu/">CS@UVa</a>
    </p>
  </div>
</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap">
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title">
            <a href="/~hw5x/Course/Text-Mining-2015-Spring/_site" title="Home">CS 6501: Text Mining</a>
            <small>Spring 2015 &middot; <a href="http://www.cs.virginia.edu/">CS@UVa</a></small>
          </h3>
        </div>
      </div>

      <div class="container content">
        <p>This assignment is designed to help you practice with general steps in building a text categorization system. It  consists of four parts, including feature selection, building Naive Bayes classifier, kNN classifier, and classification performance evaluation, totaling 100 points.</p>

<!--snippet-->

<ul>
<li><strong>Part 1: <a href="#fv">Feature Selection</a></strong> <em>(30 points)</em>: implement two basic feature selection methods, i.e., Information Gain and Chi Square test, for selecting high-quality text features;<br></li>
<li><strong>Part 2: <a href="#nb">Naive Bayes</a></strong> <em>(20 points)</em>: build a Naive Bayes classifier for text document classification;</li>
<li><strong>Part 3: <a href="#knn">k Nearest Neighbor</a></strong> <em>(20 points)</em>: build a k Nearest Neighbor classifier for text document classification;</li>
<li><strong>Part 4: <a href="#eval">Classification performance evaluation</a></strong> <em>(30 points)</em>: evaluate the constructed classifiers with standard classification performance metrics, including precision, recall and F-measure.<br></li>
</ul>

<p>The deadline and submission guidance are explicitly described <a href="#time">here</a>.</p>

<h1>Data Set</h1>

<p>We will use the same small Yelp review data set on the CS lab servers (e.g., <em>labunix01.cs.virginia.edu</em>, <em>labunix02.cs.virginia.edu</em>, <em>labunix03.cs.virginia.edu</em>):</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">&quot;/home/hw5x/TextMining/MP1/data/Yelp_Small&quot; 
</code></pre></div>
<p>More details of this review data set can be found in our previous MP1 description.</p>

<p>Please reuse your implementation for MP1 as much as possible; and based that, you should be able to finish this assignment without too much effort.</p>

<h1><a name="preparation"></a>Preparation: Creating the training and testing corpus</h1>

<h2>Terminologies</h2>

<p>We will keep using the same terminologies as we used in MP1. </p>

<ul>
<li><strong>Document</strong>: it refers to each individual Yelp review document contained in the json file. Therefore, one json file might contain multiple review documents.<br></li>
<li><strong>Restaurant</strong>: it refers to a specific restaurant entity contained in the given review data set. It is uniquely identified by the corresponding Yelp ID; and one json for one restaurant.<br></li>
<li><strong>DF/IDF/TF/TTF/Cosine similarity</strong>: the definitions for those metrics strictly follow those on our lecture slides, unless specifically defined below.</li>
</ul>

<p>Next, let&#39;s go over several important concepts and techniques for basic text analysis.</p>

<h2>Basic text processing techniques</h2>

<p>We will use the same text processing techniques as we have explored in MP1, including tokenization, stemming, normalization and stopword removal. Please find more details in our previous description of MP1. Note, in all the following assignments, we will <strong>only use unigram as our features</strong> and <strong>only use the json files in the train folder</strong>.   </p>

<h2>Defining the classes</h2>

<p>For each review document in this data set, it is associated with a numeric overall rating in the range of 1 to 5. We will treat the review documents with overall ratings <strong>below 4 as negative (labeled as 0)</strong> and <strong>the rest as positive (labeled as 1)</strong>. As a result, we have created a binary text classification problem. </p>

<h1><a name="fv"></a>Part1: Feature Selection (30pts)</h1>

<p>Based on the basic text processing steps we have taken in MP1, we have created a vector representation for every document in our collection. Next we will use two text feature selection methods, i.e., Information Gain and Chi Square, to further refine this representation.</p>

<p>The definitions of Information Gain and Chi Square feature selector are as follows,</p>

<p>$$IG(t) = -\sum_c p(c)\log p(c) + p(t)\sum_c p(c|t)\log p(c|t) + p(\head t)\sum_c p(c|\head t)\log p(c|\head t) $$,
where $p(c=1|t)$ is the probability we get a positive document where we observe the term t, and p(c|\head t) is the probability we get a positive document where we do not observe the term t.</p>

<p><strong>Note</strong>: we need to define $0\log 0=0$ to avoid zero probabilities in entropy computation.  </p>

<p>$$\chi^2(t) = \frac{(A+B+C+D)(AD-BC)}{(A+C)(B+D)(A+B)(C+D)} $$, 
where A is the number of positive documents containing term t, B is the number of positive documents not containing term t, C is the number of negative documents containing term t, and D is the number of positive documents not containing term t.</p>

<p>Before we apply these two feature selectors, we will first use DF to perform pre-filtering. We will use the same threshold as in MP1, i.e., remove the terms with DF smaller than 50. In Chi Square, we will use the threshold 3.841 to filter out the insignificant terms.</p>

<p>After this filtering steps, rank the left features by their $IG(t)$ value and $\chi^2(t)$ value in descending orders accordingly. And we will choose the top 5000 words from each of these two ordered list as our candidate features.</p>

<p>Before you move onto the next task, merge the feature list you get from Information Gain and Chi Square, i.e., take union of these two sets of top 5000 feature, and use the resulting list as the finalized features (i.e., our controlled vocabulary) for the following tasks. Then, use this controlled vocabulary to construct the vector space representation for all the documents in the collection (i.e., in the train folder). We will ignore the documents with less or equal to two non-zero features (i.e., length of the sparse vector less or equal to 2).</p>

<p><strong>What to submit</strong>:</p>

<ol>
<li>List of the top 20 words selected by Information Gain and Chi Square accordingly. What is the size of your finalized feature set after merging? How many documents are there in this feature representation? (10 pts).</li>
<li>Paste your implementation of Information Gain and Chi Square computation (2*10 pts).</li>
</ol>

<h1><a name="nb"></a>Part2: Naive Bayes (20pts)</h1>

<h3>2.1 Training Naive Bayes with maximum a posterior estimator (10pts)</h3>

<p>As we discussed in class, Naive Bayes classifier for text categorization problem boils down to estimate multiple N-gram language models under each class. In our problem, we will unigram language model with additive smoothing (set $\delta=0.1$) to estimate parameters in a Naive Bayes classifier.</p>

<p>You are suggested to reuse your implementation in MP1 for unigram language models here. </p>

<p><em>HINT: you only need to estimate two unigram language models for positive and negative classes accordingly. You can achieve that by feeding your previous implementation with different subset of documents.</em> </p>

<h1><a name="time"></a>Deadlines &amp; How to submit</h1>

<p>MP2 is due on <strong>Apr. 16th 11:55pm</strong>*. You have in total 7 days to finish this MP. The late policy for all our homework assignments has been carefully discussed in the course syllabus.</p>

<p>Collab assignment page has been created for this MP. Please submit your written report strictly following the requirement specified above. The report <strong>must be in PDF</strong> format.    </p>

      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <script>
      (function(document) {
        var toggle = document.querySelector('.sidebar-toggle');
        var sidebar = document.querySelector('#sidebar');
        var checkbox = document.querySelector('#sidebar-checkbox');

        document.addEventListener('click', function(e) {
          var target = e.target;

          if(!checkbox.checked ||
             sidebar.contains(target) ||
             (target === checkbox || target === toggle)) return;

          checkbox.checked = false;
        }, false);
      })(document);
    </script>
  </body>
</html>
